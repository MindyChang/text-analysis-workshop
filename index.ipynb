{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis from SICSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-0\"><span class=\"toc-item-num\">0&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Install-packages\" data-toc-modified-id=\"Install-packages-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Install packages</a></span></li><li><span><a href=\"#Load-packages\" data-toc-modified-id=\"Load-packages-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Load packages</a></span></li></ul></li><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Look-at-the-data-format\" data-toc-modified-id=\"Look-at-the-data-format-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Look at the data format</a></span></li><li><span><a href=\"#Look-at-individual-column-values\" data-toc-modified-id=\"Look-at-individual-column-values-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Look at individual column values</a></span></li><li><span><a href=\"#Convert-timestamps\" data-toc-modified-id=\"Convert-timestamps-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Convert timestamps</a></span></li></ul></li><li><span><a href=\"#Format-and-clean-the-text\" data-toc-modified-id=\"Format-and-clean-the-text-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Format and clean the text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter-out-retweets-and-replace-urls\" data-toc-modified-id=\"Filter-out-retweets-and-replace-urls-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Filter out retweets and replace urls</a></span></li><li><span><a href=\"#Tokenize-the-data\" data-toc-modified-id=\"Tokenize-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Tokenize the data</a></span></li><li><span><a href=\"#Convert-to-lowercase\" data-toc-modified-id=\"Convert-to-lowercase-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Convert to lowercase</a></span></li><li><span><a href=\"#Remove-punctuation\" data-toc-modified-id=\"Remove-punctuation-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Remove punctuation</a></span></li><li><span><a href=\"#Remove-stopwords\" data-toc-modified-id=\"Remove-stopwords-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Remove stopwords</a></span></li><li><span><a href=\"#Remove-numbers\" data-toc-modified-id=\"Remove-numbers-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Remove numbers</a></span></li><li><span><a href=\"#Remove-extra-white-spaces\" data-toc-modified-id=\"Remove-extra-white-spaces-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Remove extra white spaces</a></span></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Stemming</a></span></li></ul></li><li><span><a href=\"#Word-counting\" data-toc-modified-id=\"Word-counting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word counting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-word-frequencies\" data-toc-modified-id=\"Visualize-word-frequencies-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Visualize word frequencies</a></span></li><li><span><a href=\"#WordClouds\" data-toc-modified-id=\"WordClouds-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>WordClouds</a></span></li><li><span><a href=\"#Bigrams-and-n-grams\" data-toc-modified-id=\"Bigrams-and-n-grams-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Bigrams and n-grams</a></span></li><li><span><a href=\"#tf-idf:-Term-Frequency-Inverse-Document-Frequency\" data-toc-modified-id=\"tf-idf:-Term-Frequency-Inverse-Document-Frequency-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>tf-idf: Term Frequency Inverse Document Frequency</a></span></li></ul></li><li><span><a href=\"#Dictionary-based-text-analysis\" data-toc-modified-id=\"Dictionary-based-text-analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dictionary-based text analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Selecting-for-a-collection-of-words\" data-toc-modified-id=\"Selecting-for-a-collection-of-words-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Selecting for a collection of words</a></span></li><li><span><a href=\"#Sentiment-analysis\" data-toc-modified-id=\"Sentiment-analysis-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Sentiment analysis</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will step through loading and cleaning a collection of trump tweets from 2017-2018 for text analysis. In this session, we will look at word counting and dictionary-based text analysis methods.\n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.001.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "- Setup\n",
    "- Load the data\n",
    "- Format and clean the data\n",
    "- Word counting\n",
    "    - Wordclouds \n",
    "    - bigrams and n-grams\n",
    "    - tf-idf: Term frequency inverse document frequency\n",
    "- Dictionary based methods\n",
    "    - Custom dictionary\n",
    "    - Sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Some technical details:___\n",
    "\n",
    "This session is a [Binder](https://mybinder.org/) instance of a [Jupyter notebook](https://jupyter.org/) \n",
    "\n",
    "To run a code block, click the gray box and press shift+enter. After running a block, the cursor will automatically advance to the next block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "### Install packages\n",
    "We only need to install packages once - they have already been installed here.\n",
    "- *tidyverse*\n",
    "    - *dyplr* for dataframe manipulation\n",
    "    - *tidyr* for formatting into tidy data\n",
    "    - *ggplot2* for plotting\n",
    "    - *lubridate* for working with dates and times\n",
    "- *tidytext* for getting text data into a tidy format\n",
    "- *SnowballC* for getting word stems\n",
    "- *stringr* for manipulating strings\n",
    "- *wordcloud* for generating word clouds\n",
    "\n",
    "In the R console, \n",
    "```\n",
    "install.packages(\"tidyverse\")\n",
    "install.packages(\"tidytext\")\n",
    "install.packages(\"SnowballC\")\n",
    "install.packages(\"stringr\")\n",
    "install.packages(\"wordcloud\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(tidyr)\n",
    "library(ggplot2)\n",
    "library(lubridate)\n",
    "library(tidytext)\n",
    "library(SnowballC)\n",
    "library(stringr)\n",
    "library(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.002.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n",
    "\n",
    "\n",
    "We will look at Trump's tweets collected between 2017-02-05 and 2018-05-18, which has already been extracted via the twitter API using the `rtweet` package. \n",
    "This data is in the format that would be returned using an API call using the *rtweet* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(url(\"https://cbail.github.io/Trump_Tweets.Rdata\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data format\n",
    "Preview the data we loaded, which is named ```trumptweets```.\n",
    "\n",
    "You can change the number in \n",
    "```head(trumptweets, #) ```\n",
    "for the number of rows you want to see.\n",
    "\n",
    "- `created_at` contains the timestamp of the tweet\n",
    "\n",
    "- `text` contains the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview trumptweets\n",
    "head(trumptweets,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at individual column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print column names\n",
    "names(trumptweets)\n",
    "trumptweets %>%\n",
    "  select('created_at', 'text', 'favorite_count','source') %>%\n",
    "    head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ```[tablename]$[columnname]```  to select a column and apply different operations.\n",
    "Some example operations include:\n",
    "\n",
    "| operation   | Description |                      \n",
    "|-------------|-------------|\n",
    "| `min`       | minimum     |\n",
    "| `max`       | maximum     | \n",
    "| `nrow`      | # rows      | \n",
    "| `ncol`      | # columns   | \n",
    "| `unique`    | list of unique values  | \n",
    "| `n_distinct`| # unique values     | \n",
    "| `mean`| mean    | \n",
    "| `median`| median     | \n",
    "| `sd`| standard dev    | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary information on individual columns\n",
    "print(paste('# of rows: ', nrow(trumptweets)))\n",
    "min(trumptweets$created_at)\n",
    "max(trumptweets$created_at)\n",
    "unique(trumptweets$country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert timestamps \n",
    "This will make it easier to select tweets by a specific date or timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamps to timestamp format\n",
    "trumptweets$created_at <- ymd_hms(trumptweets$created_at)\n",
    "\n",
    "## examples: \n",
    "# trumptweets[as.Date(trumptweets$created_at) == as.Date(\"2018-05-18\"),]\n",
    "# trumptweets[trumptweets$created_at == ymd_hms(\"2017-05-05 19:43:37\"),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format and clean the text\n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.003.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out retweets and replace urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for parsing tweets\n",
    "replace_reg <- \"https?://[^\\\\s]+|&amp;|&lt;|&gt;|\\bRT\\\\b\"\n",
    "trumptweets <- trumptweets %>%\n",
    "  filter(is_retweet == FALSE) %>%\n",
    "  mutate(text = str_replace_all(text, replace_reg, \"url\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data\n",
    "**Tokenization** - the way you define a unit of analysis (e.g. words, sequence of words, sentence)\n",
    "\n",
    "**Document** - a unit of context (in this case - a single tweet)\n",
    "\n",
    "**Tidy text format** - One row per token (word in this case) with column variables that have extra context (e.g. which tweet the word came from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_trump_tweets<- trumptweets %>%\n",
    "    select(created_at,text) %>%\n",
    "    unnest_tokens(\"word\", text)\n",
    "\n",
    "head(tidy_trump_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lowercase\n",
    "Done automatically by `tidytext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation\n",
    "Done automatically by `tidytext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "Common words such as “the”, “and”, “for”, “is”, etc. are often described as “stop words,” meaning that they should not be included in a quantitative text analysis. The tidytext package has a list of common stop words called `stop_words` that we can use. There are also some words specific to tweets that we would like to filter out, for example urls, \"rt\" for retweets, \"t.co\" for twitter link shortening, and \"amp\" for accelerated mobile pages.\n",
    "\n",
    "Note: `tidytext` automatically converted all words to lowercase, and \"UN\" is one of the stopwords that was removed. This would likely be an important word to keep, but we'll let it slide for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stop_words from tidytext package and remove from tidy_trump_tweets\n",
    "\n",
    "#load stop_words\n",
    "data(\"stop_words\")\n",
    "# add a few more stop words\n",
    "custom_stop_words <- stop_words %>%\n",
    "    bind_rows(tibble(word = c(\"url\",\"rt\",\"t.co\",\"amp\"),\n",
    "                     lexicon = \"custom\"))\n",
    "\n",
    "# remove stopwords and other insignificant words from tidy_trump_tweets\n",
    "tidy_trump_tweets <-\n",
    "   tidy_trump_tweets %>%\n",
    "      anti_join(custom_stop_words) \n",
    "head(tidy_trump_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers\n",
    "tidy_trump_tweets<-tidy_trump_tweets[-grep(\"\\\\b\\\\d+\\\\b\", tidy_trump_tweets$word),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra white spaces\n",
    "tidy_trump_tweets$word <- gsub(\"\\\\s+\",\"\",tidy_trump_tweets$word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word stems\n",
    "tidy_trump_tweets_stemmed<-tidy_trump_tweets %>%\n",
    "      mutate_at(\"word\", list(~wordStem((.), language=\"en\")))\n",
    "head(tidy_trump_tweets_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counting \n",
    "Count the most commonly used words across tweets and plot them \n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.004.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count word frequencies and sort in descending order\n",
    "top_words<-\n",
    "   tidy_trump_tweets %>%\n",
    "    count(word) %>%\n",
    "        arrange(desc(n))\n",
    "head(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize word frequencies\n",
    "<img src=\"imgs/textanalysis_diagrams.005.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n",
    "\n",
    "Plot bar charts of word frequencies using `ggplot` from `ggplot2` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 20 most frequently used words\n",
    "plot_frequent_words <- function(word_counts) {\n",
    "    word_counts %>%\n",
    "        ggplot(aes(x=n, y=reorder(word, n), fill=-n))+\n",
    "          geom_bar(stat=\"identity\")+\n",
    "            theme_minimal()+\n",
    "            theme(axis.text.x = element_text(angle = 60, hjust = 1, size=15),\n",
    "                  axis.text.y = element_text(hjust = 1, size=15),\n",
    "                  axis.title = element_text(size=15),\n",
    "                  plot.title = element_text(hjust = 0.5, size=18))+\n",
    "                ylab(\"Frequency\")+\n",
    "                xlab(\"# Occurences\")+\n",
    "                ggtitle(\"Most Frequent Words in Trump Tweets\")+\n",
    "                guides(fill=FALSE)\n",
    "}\n",
    "\n",
    "top_words %>%\n",
    "  slice(1:20) %>%\n",
    "    plot_frequent_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds\n",
    "Create wordclouds for qualitative insights using the `wordcloud` function from the `wordcloud` package\n",
    "- `min.freq`: words with frequency below min.freq will not be plotted\n",
    "- `max.words`: Maximum number of words to be plotted. Least frequent terms are dropped\n",
    "- `random.order`: plot words in random order. If false, they will be plotted in decreasing frequency\n",
    "- `rot.per`: proportion words with 90 degree rotation\n",
    "- `colors`: color words from least to most frequent\n",
    "    - choose other color themes from [RColorBrewer](https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a wordcloud \n",
    "set.seed(1234) # for reproducibility \n",
    "wordcloud(words = top_words$word, freq = top_words$n, min.freq = 1,  \n",
    "          max.words=200, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and n-grams\n",
    "Unigrams are single words, bigrams are two-word phrases, and n-grams are n-word phrases.\n",
    "\n",
    "We can change how the tweets are tokenized to analyze n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_bigrams <-trumptweets %>%\n",
    "    select(created_at,text) %>%\n",
    "        unnest_tokens(output=word, input=text, token = \"ngrams\", n = 2) %>% \n",
    "          separate(word, c(\"word1\", \"word2\"), sep = \" \") %>% \n",
    "              filter(!word1 %in% custom_stop_words$word) %>%\n",
    "              filter(!word2 %in% custom_stop_words$word) %>% \n",
    "                  unite(word,word1, word2, sep = \" \")\n",
    "\n",
    "top_bigrams <- tidy_bigrams %>%\n",
    "    count(word) %>%\n",
    "        arrange(desc(n))\n",
    "\n",
    "top_bigrams %>%\n",
    "  slice(1:20) %>%\n",
    "    plot_frequent_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf: Term Frequency Inverse Document Frequency\n",
    "A statistic for how important a word is to a document in a collection\n",
    "\n",
    "Words that occur more frequently in one document (tweet) and less frequently in other documents should be given more importance as they are more useful for classification.\n",
    "\n",
    "***Term frequency***\n",
    "\n",
    "$tf(term)=\\displaystyle(\\frac{n_{occurences\\ of\\ term\\ in\\ document}}{n_{words\\ in\\ document}})$\n",
    "\n",
    "***Inverse document Frequency:***\n",
    "\n",
    "$idf(term)=\\displaystyle log(\\frac{n_{documents}}{n_{documents\\ containing\\ term}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_trump_tfidf <- tidy_trump_tweets %>%\n",
    "    count(word, created_at) %>%\n",
    "        bind_tf_idf(word, created_at, n) %>%\n",
    "            arrange(desc(tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234) # for reproducibility \n",
    "tidy_trump_tfidf_unique <- tidy_trump_tfidf %>%\n",
    "    distinct(word,.keep_all = TRUE)\n",
    "wordcloud(words = tidy_trump_tfidf_unique$word, freq = tidy_trump_tfidf_unique$tf_idf, min.freq = .5,  \n",
    "          max.words=100, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"),scale=c(2,.5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary-based text analysis\n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.006.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting for a collection of words\n",
    "We can create a custom list of words and find tweets that contain any of those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dictionary<-c(\"economy\",\"unemployment\",\"trade\",\"tariffs\",\"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "custom_dictionary_tweets<-trumptweets[str_detect(trumptweets$text, \n",
    "                                                regex(paste(custom_dictionary, collapse=\"|\"),\n",
    "                                                      ignore_case=TRUE)),]\n",
    "custom_dictionary_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a wordcloud for our tweets that match our custom dictionary\n",
    "custom_top_words<-custom_dictionary_tweets %>%\n",
    "    select(created_at,text) %>%\n",
    "      unnest_tokens(\"word\", text) %>%\n",
    "        anti_join(stop_words) %>%\n",
    "            filter(!(word==\"https\"|\n",
    "                 word==\"rt\"|\n",
    "                 word==\"t.co\"|\n",
    "                 word==\"amp\" |\n",
    "                 word==\"url\")) %>%\n",
    "            count(word) %>%\n",
    "                arrange(desc(n))\n",
    "set.seed(1234) # for reproducibility \n",
    "wordcloud(words = custom_top_words$word, freq = custom_top_words$n, min.freq = 1,  \n",
    "          max.words=100, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"),scale=c(4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular type of dictionary is a sentiment dictionary which can be used to assess the valence of a given text by searching for words that describe affect or opinion. \n",
    "\n",
    "`tidytext` has a few built-in sentiment dictionaries\n",
    "- `afinn` - sentiment words in twitter discussions of climate change (value between -5 and 5)\n",
    "- `bing` - sentiment words identified in online forums (negative vs positive)\n",
    "- `nrc` - created emotional valence words by mturk workers\n",
    "\n",
    "Let's use `nrc`. \n",
    "- Words in this dictionary are labeled with the sentiments: \"negative\",\"positive\",\"trust\",\"fear\",\"sadness\",\"anger\", \"surprise\",\"disgust\",\"joy\",\"anticipation\"\n",
    "- Each word can be associated with multiple sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nrc dictionary, look at unique sentiments and an example word for each\n",
    "head(get_sentiments(\"nrc\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of sentiment words for each tweet\n",
    "trump_tweet_sentiment <- tidy_trump_tweets %>%\n",
    "  inner_join(get_sentiments(\"nrc\")) %>%\n",
    "    count(created_at, sentiment) \n",
    "head(trump_tweet_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group sentiment counts by tweet\n",
    "trump_sentiment_spread <-trump_tweet_sentiment %>%\n",
    "  spread(sentiment, n, fill=0)\n",
    "\n",
    "trump_sentiment_full <- merge(trump_sentiment_spread, \n",
    "                              trumptweets[c(\"created_at\",\"text\",\"favorite_count\")], \n",
    "                              by=\"created_at\")\n",
    "head(trump_sentiment_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments <- as.list(get_sentiments(\"nrc\") %>% distinct(sentiment))\n",
    "for (s in sentiments)\n",
    "{\n",
    "    print(s)    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 <-trump_sentiment_full %>%\n",
    "  lm(data=., favorite_count ~ disgust + negative + joy + anticipation + positive + sadness + trust + fear)\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
