{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis Workshop\n",
    "Based on [SICSS 2020](https://compsocialscience.github.io/summer-institute/curriculum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-0\"><span class=\"toc-item-num\">0&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Install-packages\" data-toc-modified-id=\"Install-packages-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Install packages</a></span></li><li><span><a href=\"#Load-packages\" data-toc-modified-id=\"Load-packages-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Load packages</a></span></li></ul></li><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Look-at-the-data-format\" data-toc-modified-id=\"Look-at-the-data-format-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Look at the data format</a></span></li><li><span><a href=\"#Look-at-individual-column-values\" data-toc-modified-id=\"Look-at-individual-column-values-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Look at individual column values</a></span></li></ul></li><li><span><a href=\"#Format-and-clean-the-text\" data-toc-modified-id=\"Format-and-clean-the-text-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Format and clean the text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter-out-retweets-and-replace-urls\" data-toc-modified-id=\"Filter-out-retweets-and-replace-urls-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Filter out retweets and replace urls</a></span></li><li><span><a href=\"#Tokenize-the-data\" data-toc-modified-id=\"Tokenize-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Tokenize the data</a></span></li><li><span><a href=\"#Convert-to-lowercase\" data-toc-modified-id=\"Convert-to-lowercase-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Convert to lowercase</a></span></li><li><span><a href=\"#Remove-punctuation\" data-toc-modified-id=\"Remove-punctuation-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Remove punctuation</a></span></li><li><span><a href=\"#Remove-stopwords\" data-toc-modified-id=\"Remove-stopwords-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Remove stopwords</a></span></li><li><span><a href=\"#Remove-numbers\" data-toc-modified-id=\"Remove-numbers-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Remove numbers</a></span></li><li><span><a href=\"#Remove-extra-white-spaces\" data-toc-modified-id=\"Remove-extra-white-spaces-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Remove extra white spaces</a></span></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Stemming</a></span></li></ul></li><li><span><a href=\"#Word-counting\" data-toc-modified-id=\"Word-counting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word counting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-word-frequencies\" data-toc-modified-id=\"Visualize-word-frequencies-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Visualize word frequencies</a></span></li><li><span><a href=\"#WordClouds\" data-toc-modified-id=\"WordClouds-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>WordClouds</a></span></li><li><span><a href=\"#Bigrams-and-n-grams\" data-toc-modified-id=\"Bigrams-and-n-grams-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Bigrams and n-grams</a></span></li><li><span><a href=\"#tf-idf:-Term-Frequency-Inverse-Document-Frequency\" data-toc-modified-id=\"tf-idf:-Term-Frequency-Inverse-Document-Frequency-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>tf-idf: Term Frequency Inverse Document Frequency</a></span></li></ul></li><li><span><a href=\"#Dictionary-based-text-analysis\" data-toc-modified-id=\"Dictionary-based-text-analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dictionary-based text analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Selecting-for-keywords\" data-toc-modified-id=\"Selecting-for-keywords-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Selecting for keywords</a></span></li><li><span><a href=\"#Sentiment-analysis\" data-toc-modified-id=\"Sentiment-analysis-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Sentiment analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Select-a-dictionary\" data-toc-modified-id=\"Select-a-dictionary-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Select a dictionary</a></span></li><li><span><a href=\"#Count-sentiment-words\" data-toc-modified-id=\"Count-sentiment-words-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Count sentiment words</a></span></li></ul></li><li><span><a href=\"#Count-sentiments-by-tweet\" data-toc-modified-id=\"Count-sentiments-by-tweet-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Count sentiments by tweet</a></span><ul class=\"toc-item\"><li><span><a href=\"#Examine-sentiment-labels\" data-toc-modified-id=\"Examine-sentiment-labels-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Examine sentiment labels</a></span></li><li><span><a href=\"#Plot-sentiments-over-time\" data-toc-modified-id=\"Plot-sentiments-over-time-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Plot sentiments over time</a></span></li><li><span><a href=\"#Linear-model-for-favorites-based-on-sentiment-count\" data-toc-modified-id=\"Linear-model-for-favorites-based-on-sentiment-count-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Linear model for favorites based on sentiment count</a></span></li></ul></li><li><span><a href=\"#Other-dictionaries\" data-toc-modified-id=\"Other-dictionaries-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Other dictionaries</a></span></li></ul></li><li><span><a href=\"#Appendix\" data-toc-modified-id=\"Appendix-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Appendix</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will step through loading and cleaning a collection of Trump tweets from 2017-2018 for text analysis. In this session, we will look at word counting and dictionary-based text analysis methods like sentiment analysis.\n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.001.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Some operational details:___\n",
    "\n",
    "This session is a [Binder](https://mybinder.org/) instance of a [Jupyter notebook](https://jupyter.org/) \n",
    "\n",
    "To run a code block, click the gray box and press shift+enter. After running a block, the cursor will automatically advance to the next block.\n",
    "\n",
    "The binder session will time out after a period of inactivity. If this happens, you will need to restart the binder. Reloading the current page will not work. \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/MindyChang/text-analysis-workshop/master?filepath=index.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "### Install packages\n",
    "\n",
    "We only need to install packages once - they have already been installed in this binder.\n",
    "- *tidyverse*\n",
    "    - *dyplr* for dataframe manipulation\n",
    "    - *tidyr* for formatting into tidy data\n",
    "    - *ggplot2* for plotting\n",
    "    - *lubridate* for working with dates and times\n",
    "- *tidytext* for getting text data into a tidy format\n",
    "- *SnowballC* for getting word stems\n",
    "- *stringr* for manipulating strings\n",
    "- *wordcloud* for generating word clouds\n",
    "\n",
    "In the R console, \n",
    "```\n",
    "install.packages(\"tidyverse\")\n",
    "install.packages(\"tidytext\")\n",
    "install.packages(\"SnowballC\")\n",
    "install.packages(\"stringr\")\n",
    "install.packages(\"wordcloud\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(tidyr)\n",
    "library(ggplot2)\n",
    "library(lubridate)\n",
    "library(tidytext)\n",
    "library(SnowballC)\n",
    "library(stringr)\n",
    "library(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.002.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n",
    "\n",
    "\n",
    "We will look at Trump's tweets collected between 2017-02-05 and 2018-05-18, which has already been extracted via the twitter API using the `rtweet` package and saved as a Rdata file. \n",
    "\n",
    "The data is named `trumptweets`, and we will rename it by assigning the variable name `rawtweets` using the assignment operator `<-`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(url(\"https://cbail.github.io/Trump_Tweets.Rdata\"))\n",
    "\n",
    "rawtweets <- trumptweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data format\n",
    "The data is formatted as a data frame, which is a standard table, where the top row contains column names and each row contains data about a single tweet.\n",
    "\n",
    "The `head()`function returns a the first few rows of `rawtweets`. \n",
    "\n",
    "You can change the number in \n",
    "`head(rawtweets, #) `\n",
    "for the number of rows you want to see.\n",
    "\n",
    "- `created_at` contains the timestamp of the tweet\n",
    "\n",
    "- `text` contains the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview rawtweets\n",
    "head(rawtweets,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at individual column values\n",
    "The `names()` function lists the column names\n",
    "\n",
    "\n",
    "The `select()` function lets us select which columns we want to output\n",
    "\n",
    "The pipe operation `%>%` provides an elegant way to sequentially pass the data through different operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print column names\n",
    "names(rawtweets)\n",
    "# display selected columns\n",
    "rawtweets %>%\n",
    "  select('created_at', 'text', 'favorite_count','source') %>%\n",
    "    head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `$` operation selects a column like this: `[tablename]$[columnname]`\n",
    "\n",
    "We can apply different operations to columns. Some example operations include:\n",
    "\n",
    "| operation   | Description |                      \n",
    "|-------------|-------------|\n",
    "| `min`       | minimum     |\n",
    "| `max`       | maximum     | \n",
    "| `nrow`      | # rows      | \n",
    "| `ncol`      | # columns   | \n",
    "| `unique`    | list of unique values  | \n",
    "| `n_distinct`| # unique values     | \n",
    "| `mean`| mean    | \n",
    "| `median`| median     | \n",
    "| `sd`| standard dev    | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary information on individual columns\n",
    "min(rawtweets$created_at)\n",
    "max(rawtweets$created_at)\n",
    "unique(rawtweets$country)\n",
    "print(paste('# of rows: ', nrow(rawtweets)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format and clean the text\n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.003.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out retweets and replace urls\n",
    "The `filter()` function selects for rows that fit the specified criteria.\n",
    "\n",
    "The `mutate()` function specifies how to overwrite or create a new column based on existing columns.\n",
    "\n",
    "The `str_replace_all()` function finds strings that match the specified criteria and replaces each one. Below, we are finding all strings that look like urls and replacing them with the string \"url\".\n",
    "\n",
    "A Regular expression, or regex, is sequence of characters that define a search pattern. GREP, which stands for “Globally search a Regular Expression and Print”, is a useful function for searching through text. Here a [useful cheatsheet](https://rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf) for creating a regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out retweets, find urls and replace them with the string \"url\"\n",
    "# regex for parsing tweets\n",
    "replace_reg <- \"https?://[^\\\\s]+|&amp;|&lt;|&gt;|\\bRT\\\\b\"\n",
    "rawtweets <- rawtweets %>%\n",
    "  filter(is_retweet == FALSE) %>%\n",
    "  mutate(text = str_replace_all(text, replace_reg, \"url\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data\n",
    "\n",
    "**Tokenization** - the way you define a unit of analysis (e.g. words, sequence of words, sentence)\n",
    "\n",
    "**Document** - a unit of context (in this case - a single tweet)\n",
    "\n",
    "**Tidy text format** - One row per token (word in this case) with column variables that have extra context (e.g. which document the word came from)\n",
    "\n",
    "The `unnest_tokens()` function separates the tweets into a tidy text format using the token specified. Here we are defining a token as a single word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets<- rawtweets %>%\n",
    "    select(created_at,text) %>%\n",
    "    unnest_tokens(\"word\", text)\n",
    "\n",
    "head(tidy_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lowercase\n",
    "Done automatically by `unnest_tokens` from `tidytext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation\n",
    "Done automatically by `unnest_tokens` from `tidytext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "Common words such as “the”, “and”, “for”, “is”, etc. are often described as “stop words,” meaning that they should not be included in a text analysis. The `tidytext` package has a list of common stop words called `stop_words` that we can use. There are also some words specific to tweets that we would like to filter out, for example urls, \"rt\" for retweets, \"t.co\" for twitter link shortening, and \"amp\" for accelerated mobile pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stop_words from tidytext package and remove them from tidy_tweets\n",
    "\n",
    "#load stop_words\n",
    "data(\"stop_words\")\n",
    "# add a few more stop words\n",
    "custom_stop_words <- stop_words %>%\n",
    "    bind_rows(tibble(word = c(\"url\",\"rt\",\"t.co\",\"amp\"),\n",
    "                     lexicon = \"custom\"))\n",
    "\n",
    "# remove stopwords and other insignificant words from tidy_tweets\n",
    "tidy_tweets <-\n",
    "   tidy_tweets %>%\n",
    "      anti_join(custom_stop_words) \n",
    "head(tidy_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We might need to go back and edit our stopwords that have alternative meanings, for example:\n",
    "- `tidytext` automatically converted all words to lowercase, and removed \"UN\" \n",
    "- `tidytext` automatically removed punctuation, and \"Secretary-General\" was reduced to only \"secretary\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers from tidy_tweets\n",
    "tidy_tweets<-tidy_tweets[-grep(\"\\\\b\\\\d+\\\\b\", tidy_tweets$word),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra white spaces from tidy_tweets\n",
    "tidy_tweets$word <- gsub(\"\\\\s+\",\"\",tidy_tweets$word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "We may want to reduce all words to their word stems. For example: \"unite\", \"united\", \"uniting\", \"unites\" all reduce to unit. The code below shows how to do it, but we won't use it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word stems and save as tidy_tweets_stemmed\n",
    "tidy_tweets_stemmed<-tidy_tweets %>%\n",
    "      mutate_at(\"word\", list(~wordStem((.), language=\"en\")))\n",
    "head(tidy_tweets_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counting \n",
    "Count the most commonly used words across tweets and plot them \n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.004.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count word frequencies and sort in descending order\n",
    "top_words<-\n",
    "   tidy_tweets %>%\n",
    "    count(word) %>%\n",
    "        arrange(desc(n))\n",
    "head(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize word frequencies\n",
    "<img src=\"imgs/textanalysis_diagrams.005.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n",
    "\n",
    "Plot bar charts of word frequencies using `ggplot` from `ggplot2` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 20 most frequently used words\n",
    "# function for bar plots given a dataframe with columns [word,n]\n",
    "plot_frequent_words <- function(word_counts) {\n",
    "    word_counts %>%\n",
    "        ggplot(aes(x=n, y=reorder(word, n), fill=-n))+\n",
    "          geom_bar(stat=\"identity\")+\n",
    "            theme_minimal()+\n",
    "            theme(axis.text.x = element_text(angle = 60, hjust = 1, size=15),\n",
    "                  axis.text.y = element_text(hjust = 1, size=15),\n",
    "                  axis.title = element_text(size=15),\n",
    "                  plot.title = element_text(hjust = 0.5, size=18))+\n",
    "                ylab(\"Frequency\")+\n",
    "                xlab(\"# Occurences\")+\n",
    "                ggtitle(\"Most Frequent Words in Trump Tweets\")+\n",
    "                guides(fill=FALSE)\n",
    "}\n",
    "\n",
    "top_words %>%\n",
    "  slice(1:20) %>%\n",
    "    plot_frequent_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds\n",
    "Create wordclouds for qualitative insights using the `wordcloud` function from the `wordcloud` package\n",
    "- `min.freq`: words with frequency below min.freq will not be plotted\n",
    "- `max.words`: Maximum number of words to be plotted. Least frequent terms are dropped\n",
    "- `random.order`: plot words in random order. If false, they will be plotted in decreasing frequency\n",
    "- `rot.per`: proportion words with 90 degree rotation\n",
    "- `colors`: color words from least to most frequent\n",
    "    - choose other color themes from [RColorBrewer](https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a wordcloud \n",
    "set.seed(1234) # for reproducibility \n",
    "wordcloud(words = top_words$word, freq = top_words$n, min.freq = 1,  \n",
    "          max.words=200, random.order=FALSE, rot.per=0.3,colors=brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and n-grams\n",
    "\n",
    "Bigrams and n-grams refer to how text is tokenized, or the size of the unit of analysis.\n",
    "\n",
    "Unigrams are single words, bigrams are two-word phrases, and n-grams are n-word phrases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess rawtweets by tokenizing into bigrams, removing stopwords from individual words,\n",
    "# and then combining back together\n",
    "tidy_bigrams <-rawtweets %>%\n",
    "    select(created_at,text) %>%\n",
    "        unnest_tokens(output=word, input=text, token = \"ngrams\", n = 2) %>% \n",
    "          separate(word, c(\"word1\", \"word2\"), sep = \" \") %>% \n",
    "              filter(!word1 %in% custom_stop_words$word) %>%\n",
    "              filter(!word2 %in% custom_stop_words$word) %>% \n",
    "                  unite(word,word1, word2, sep = \" \")\n",
    "\n",
    "# count bigrams and arrange by frequency\n",
    "top_bigrams <- tidy_bigrams %>%\n",
    "    count(word) %>%\n",
    "        arrange(desc(n))\n",
    "\n",
    "# plot top bigrams\n",
    "top_bigrams %>%\n",
    "  slice(1:20) %>%\n",
    "    plot_frequent_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf: Term Frequency Inverse Document Frequency\n",
    "A statistic for how important a word is to a document in a collection\n",
    "\n",
    "Words that occur more frequently in one document (tweet) and less frequently in other documents should be given more importance as they are more useful for classification.\n",
    "\n",
    "***Term frequency***\n",
    "\n",
    "$tf(term)=\\displaystyle(\\frac{n_{occurences\\ of\\ term\\ in\\ document}}{n_{words\\ in\\ document}})$\n",
    "\n",
    "***Inverse document Frequency:***\n",
    "\n",
    "$idf(term)=\\displaystyle log(\\frac{n_{documents}}{n_{documents\\ containing\\ term}})$\n",
    "\n",
    "The `bind_tf_idf` function from the `tidytext` package calculates the tf-idf value for each token (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets_tfidf <- tidy_tweets %>%\n",
    "    count(word, created_at) %>%\n",
    "        bind_tf_idf(word, created_at, n) %>%\n",
    "            arrange(desc(tf_idf)) %>%\n",
    "                distinct(word,.keep_all = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234) # for reproducibility     \n",
    "wordcloud(words = tidy_tweets_tfidf$word, freq = tidy_tweets_tfidf$tf_idf, min.freq = .5,  \n",
    "          max.words=100, random.order=FALSE, rot.per=0.3,colors=brewer.pal(8, \"Dark2\"),scale=c(2,.5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary-based text analysis\n",
    "\n",
    "<img src=\"imgs/textanalysis_diagrams.006.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting for keywords\n",
    "We can create a custom list of words and find tweets that contain any of those words\n",
    "\n",
    "The `str_detect` function from `stringr` package finds all text that contains a specified string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of words as our custom dictionary\n",
    "custom_dictionary<-c(\"economy\",\"unemployment\",\"trade\",\"tariffs\",\"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tweets that contain any words from our custom dictionary\n",
    "custom_dictionary_tweets<-rawtweets[str_detect(rawtweets$text, \n",
    "                                                regex(paste(custom_dictionary, collapse=\"|\"),\n",
    "                                                      ignore_case=TRUE)),]\n",
    "custom_dictionary_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a wordcloud for our tweets that match our custom dictionary\n",
    "custom_top_words<-custom_dictionary_tweets %>%\n",
    "    select(created_at,text) %>%\n",
    "      unnest_tokens(\"word\", text) %>%\n",
    "        anti_join(custom_stop_words) %>%\n",
    "            count(word) %>%\n",
    "                arrange(desc(n))\n",
    "\n",
    "#plot wordcloud\n",
    "set.seed(1234) # for reproducibility \n",
    "wordcloud(words = custom_top_words$word, freq = custom_top_words$n, min.freq = 1,  \n",
    "          max.words=100, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"),scale=c(4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis\n",
    "One popular type of dictionary is a sentiment dictionary which can be used to assess the valence of a given text by searching for words that describe affect or opinion. \n",
    "Sentiment dictionaries can vary highly. See [this paper]( https://homepages.dcc.ufmg.br/~fabricio/download/cosn127-goncalves.pdf) for a comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a dictionary\n",
    "\n",
    "`tidytext` has a few built-in sentiment dictionaries\n",
    "- `afinn` - sentiment words in twitter discussions of climate change (value between -5 and 5)\n",
    "- `bing` - sentiment words identified in online forums (negative vs positive)\n",
    "- `nrc` - emotional valence words labeled by mturk workers\n",
    "    - Words in this dictionary are labeled with the sentiments:\n",
    "    \"negative\",\"positive\",\"trust\",\"fear\",\"sadness\",\"anger\", \"surprise\",\"disgust\",\"joy\",\"anticipation\"\n",
    "    - Each word can be associated with multiple sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the bing dictionary\n",
    "sentiment_dictionary = \"bing\"\n",
    "head(get_sentiments(sentiment_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count sentiment words\n",
    "We use the `inner_join` function to count the number of sentiment words used across all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_by_word <- tidy_tweets %>%\n",
    "  inner_join(get_sentiments(sentiment_dictionary)) %>%\n",
    "    count(word, sentiment) \n",
    "\n",
    "head(sentiments_by_word)\n",
    "\n",
    "tweet_sentiments_by_word %>%\n",
    "  group_by(sentiment) %>%\n",
    "  top_n(10) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(word = reorder(word, n)) %>%\n",
    "  ggplot(aes(word, n, fill = sentiment)) +\n",
    "    theme_minimal()+\n",
    "    theme(axis.text.x = element_text(angle = 60, hjust = 1, size=15),\n",
    "          axis.text.y = element_text(hjust = 1, size=15),\n",
    "          axis.title = element_text(size=15),\n",
    "          plot.title = element_text(hjust = 0.5, size=18),\n",
    "          strip.text = element_text(size=18))+\n",
    "          \n",
    "    theme(aspect.ratio = 1/1.5)+\n",
    "    geom_col(show.legend = FALSE, width = 0.5) +  \n",
    "    facet_wrap(~sentiment, scales = \"free_y\") +\n",
    "    labs(y = \"Contribution to sentiment\",\n",
    "         x = NULL) +\n",
    "    coord_flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \"trump\" happens to be a positive sentiment word, but it's being used in a different way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count sentiments by tweet\n",
    "This time we will use the `inner_join` function to count the number of sentiment words per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of sentiment words for each tweet\n",
    "tweet_sentiments <- tidy_tweets %>%\n",
    "  inner_join(get_sentiments(sentiment_dictionary)) %>%\n",
    "    count(created_at, sentiment) \n",
    "head(tweet_sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group sentiment counts by tweet\n",
    "tweet_sentiments_spread <-tweet_sentiments %>%\n",
    "  spread(sentiment, n, fill=0)\n",
    "\n",
    "# add in the original text and favorite count\n",
    "tweet_sentiments_full <- merge(tweet_sentiments_spread, \n",
    "                              rawtweets[c(\"created_at\",\"text\",\"favorite_count\")], \n",
    "                              by=\"created_at\")\n",
    "head(tweet_sentiments_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine sentiment labels\n",
    "Let's see what the tweets with the highest counts for each sentiment look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentiment, print a tweet with the highest count for that sentiment\n",
    "sentiments <- as.list(get_sentiments(sentiment_dictionary) \n",
    "                      %>% distinct(sentiment))\n",
    "\n",
    "high_sentiments <- tweet_sentiments_full[FALSE,] \n",
    "high_sentiments$highest <- NULL \n",
    "\n",
    "for (s in sentiments[[1]])\n",
    "{\n",
    "    tmp <- tweet_sentiments_full %>% \n",
    "            arrange(desc(get(s)))\n",
    "    tmp$highest <- s\n",
    "    high_sentiments <- high_sentiments %>%\n",
    "        bind_rows(tmp[1,])\n",
    "}\n",
    "high_sentiments <- high_sentiments %>%\n",
    "    select(-one_of('created_at', 'favorite_count'))\n",
    "high_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word clouds for the top 50 most negative and top 50 most positive tweets\n",
    "for (s in c(\"negative\",\"positive\"))\n",
    "{\n",
    "    print(s)\n",
    "    top_words <- tweet_sentiments_full %>% \n",
    "        arrange(desc(get(s))) %>%\n",
    "        slice(1:50) %>%\n",
    "        select(created_at,text) %>%\n",
    "          unnest_tokens(\"word\", text) %>%\n",
    "            anti_join(custom_stop_words) %>%\n",
    "                count(word) %>%\n",
    "                    arrange(desc(n))\n",
    "\n",
    "    #plot wordcloud\n",
    "    set.seed(1234) # for reproducibility \n",
    "    wordcloud(words = top_words$word, freq = top_words$n, min.freq = 2,  \n",
    "              max.words=100, random.order=FALSE, rot.per=0.3,colors=brewer.pal(8, \"Dark2\"),scale=c(4,.2))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot sentiments over time\n",
    "A note about timestamps: The `lubridate` package provides functions like `as.Date` and `ymd_hms` to deal with timestamp formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert timestamps to timestamp format\n",
    "#rawtweets$created_at <- ymd_hms(rawtweets$created_at)\n",
    "\n",
    "## examples: \n",
    "# rawtweets[as.Date(rawtweets$created_at) == as.Date(\"2018-05-18\"),]\n",
    "# rawtweets[rawtweets$created_at == ymd_hms(\"2017-05-05 19:43:37\"),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get date for each tweet\n",
    "tweet_sentiments_full$date <- as.Date(tweet_sentiments_full$created_at, format=\"%Y-%m-%d %x\")\n",
    "\n",
    "# plot count of negative and positive sentiment words by date\n",
    "options(repr.plot.width=12, repr.plot.height=10)\n",
    "tweet_sentiments_full %>%\n",
    "    group_by(date) %>%\n",
    "    summarise_at(vars(positive,negative), list(n = sum)) %>%\n",
    "    ggplot(aes(x = date)) + \n",
    "      geom_line(aes(y = positive_n, color = \"Positive\")) +\n",
    "      geom_line(aes(y = negative_n, color = \"Negative\")) +        \n",
    "        scale_color_manual(values = c(\n",
    "            'Positive' = 'green',\n",
    "            'Negative' = 'red')) +\n",
    "        theme_minimal()+\n",
    "        theme(axis.text.x = \n",
    "            element_text(angle = 60, hjust = 1, size=13))+\n",
    "        theme(plot.title = \n",
    "            element_text(hjust = 0.5, size=18))+\n",
    "          ylab(\"Number of Sentiment Words\")+\n",
    "          xlab(\"\")+\n",
    "          labs(color = \"Sentiment\") +\n",
    "          ggtitle(\"Positive vs Negative Sentiment in Trump Tweets\")+\n",
    "          theme(aspect.ratio=1/4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear model for favorites based on sentiment count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 <-tweet_sentiments_full %>%\n",
    "    lm(data=., favorite_count ~  negative + positive)\n",
    "    ## for nrc only\n",
    "    #lm(data=., favorite_count ~  negative + positive + disgust +sadness + trust + fear + joy + anticipation)\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other dictionaries\n",
    "\n",
    "**Linguistic Inquiry Word Count (LIWC)** - a large, proprietary dictionary developed by social psychologist to classify different types of psychometric properties and substantive properties of a text. More details [here](http://liwc.wpengine.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "507.778px",
    "left": "23px",
    "top": "312.052px",
    "width": "284.444px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
