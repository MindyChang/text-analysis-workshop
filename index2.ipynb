{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis Workshop Part 2\n",
    "\n",
    "\n",
    "\n",
    "Based on [SICSS 2020](https://compsocialscience.github.io/summer-institute/curriculum) (Summer Institute in Computational Social Science)\n",
    "\n",
    "- [Basic Text Analysis in R](https://compsocialscience.github.io/summer-institute/2020/materials/day3-text-analysis/basic-text-analysis/rmarkdown/Basic_Text_Analysis_in_R.html)\n",
    "- [Dictionary-based Text Analysis](https://compsocialscience.github.io/summer-institute/2020/materials/day3-text-analysis/dictionary-methods/rmarkdown/Dictionary-Based_Text_Analysis.html#when-should-i-use-a-dictionary-based-approach)\n",
    "- [Topic Modeling](https://cbail.github.io/SICSS_Topic_Modeling.html)\n",
    "- [Text Mining with R](https://www.tidytextmining.com/)\n",
    "\n",
    "Mindy Chang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-0\"><span class=\"toc-item-num\">0&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Install-packages\" data-toc-modified-id=\"Install-packages-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Install packages</a></span></li><li><span><a href=\"#Load-packages\" data-toc-modified-id=\"Load-packages-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Load packages</a></span></li></ul></li><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-Twitter-data\" data-toc-modified-id=\"Getting-Twitter-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Getting Twitter data</a></span></li><li><span><a href=\"#Look-at-the-data-format\" data-toc-modified-id=\"Look-at-the-data-format-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Look at the data format</a></span></li></ul></li><li><span><a href=\"#Format-and-clean-the-text\" data-toc-modified-id=\"Format-and-clean-the-text-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Format and clean the text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tweet-specific-cleaning\" data-toc-modified-id=\"Tweet-specific-cleaning-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Tweet-specific cleaning</a></span></li><li><span><a href=\"#Tokenize-the-data\" data-toc-modified-id=\"Tokenize-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Tokenize the data</a></span></li><li><span><a href=\"#Convert-to-lowercase\" data-toc-modified-id=\"Convert-to-lowercase-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Convert to lowercase</a></span></li><li><span><a href=\"#Remove-punctuation\" data-toc-modified-id=\"Remove-punctuation-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Remove punctuation</a></span></li><li><span><a href=\"#Remove-stopwords\" data-toc-modified-id=\"Remove-stopwords-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Remove stopwords</a></span></li><li><span><a href=\"#Remove-numbers\" data-toc-modified-id=\"Remove-numbers-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Remove numbers</a></span></li><li><span><a href=\"#Remove-extra-white-spaces\" data-toc-modified-id=\"Remove-extra-white-spaces-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Remove extra white spaces</a></span></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Stemming</a></span></li></ul></li><li><span><a href=\"#Word-counting\" data-toc-modified-id=\"Word-counting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word counting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-word-count-dataframe\" data-toc-modified-id=\"Create-a-word-count-dataframe-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create a word count dataframe</a></span></li><li><span><a href=\"#Visualize-word-frequencies\" data-toc-modified-id=\"Visualize-word-frequencies-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Visualize word frequencies</a></span></li><li><span><a href=\"#WordClouds\" data-toc-modified-id=\"WordClouds-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>WordClouds</a></span></li><li><span><a href=\"#Bigrams-and-n-grams\" data-toc-modified-id=\"Bigrams-and-n-grams-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Bigrams and n-grams</a></span></li><li><span><a href=\"#tf-idf:-Term-Frequency-Inverse-Document-Frequency\" data-toc-modified-id=\"tf-idf:-Term-Frequency-Inverse-Document-Frequency-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>tf-idf: Term Frequency Inverse Document Frequency</a></span></li></ul></li><li><span><a href=\"#Dictionary-based-text-analysis\" data-toc-modified-id=\"Dictionary-based-text-analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dictionary-based text analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Select-for-keywords\" data-toc-modified-id=\"Select-for-keywords-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Select for keywords</a></span></li><li><span><a href=\"#Sentiment-analysis\" data-toc-modified-id=\"Sentiment-analysis-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Sentiment analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-a-dictionary\" data-toc-modified-id=\"Load-a-dictionary-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Load a dictionary</a></span></li><li><span><a href=\"#Count-sentiment-words-across-tweets\" data-toc-modified-id=\"Count-sentiment-words-across-tweets-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Count sentiment words across tweets</a></span></li><li><span><a href=\"#Count-sentiments-by-tweet\" data-toc-modified-id=\"Count-sentiments-by-tweet-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Count sentiments by tweet</a></span></li><li><span><a href=\"#Examine-sentiment-labels\" data-toc-modified-id=\"Examine-sentiment-labels-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Examine sentiment labels</a></span></li><li><span><a href=\"#[extra]-Plot-sentiments-over-time\" data-toc-modified-id=\"[extra]-Plot-sentiments-over-time-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>[extra] Plot sentiments over time</a></span></li><li><span><a href=\"#[extra]-Linear-model-for-favorites-based-on-sentiment-count\" data-toc-modified-id=\"[extra]-Linear-model-for-favorites-based-on-sentiment-count-4.2.6\"><span class=\"toc-item-num\">4.2.6&nbsp;&nbsp;</span>[extra] Linear model for favorites based on sentiment count</a></span></li></ul></li><li><span><a href=\"#Linguistic-Inquiry-Word-Count-(LIWC)\" data-toc-modified-id=\"Linguistic-Inquiry-Word-Count-(LIWC)-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Linguistic Inquiry Word Count (LIWC)</a></span></li></ul></li><li><span><a href=\"#Topic-Models\" data-toc-modified-id=\"Topic-Models-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Topic Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#LDA-(Latent-dirichlet-allocation)\" data-toc-modified-id=\"LDA-(Latent-dirichlet-allocation)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>LDA (Latent dirichlet allocation)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Document-term-matrix\" data-toc-modified-id=\"Document-term-matrix-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Document-term matrix</a></span></li><li><span><a href=\"#Create-LDA-topic-model\" data-toc-modified-id=\"Create-LDA-topic-model-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Create LDA topic model</a></span></li><li><span><a href=\"#Top-terms-in-each-topic\" data-toc-modified-id=\"Top-terms-in-each-topic-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Top terms in each topic</a></span></li><li><span><a href=\"#Top-documents-in-each-topic\" data-toc-modified-id=\"Top-documents-in-each-topic-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Top documents in each topic</a></span></li></ul></li><li><span><a href=\"#STM-(Structural-Topic-Modeling)\" data-toc-modified-id=\"STM-(Structural-Topic-Modeling)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>STM (Structural Topic Modeling)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Load the data</a></span></li><li><span><a href=\"#Process-the-data\" data-toc-modified-id=\"Process-the-data-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Process the data</a></span></li><li><span><a href=\"#Create-STM-model\" data-toc-modified-id=\"Create-STM-model-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Create STM model</a></span></li><li><span><a href=\"#Top-documents-in-each-topic\" data-toc-modified-id=\"Top-documents-in-each-topic-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Top documents in each topic</a></span></li><li><span><a href=\"#Effect-of-dataset-(date)-on-topic-distribution\" data-toc-modified-id=\"Effect-of-dataset-(date)-on-topic-distribution-5.2.5\"><span class=\"toc-item-num\">5.2.5&nbsp;&nbsp;</span>Effect of dataset (date) on topic distribution</a></span></li></ul></li><li><span><a href=\"#Topic-model-notes\" data-toc-modified-id=\"Topic-model-notes-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Topic model notes</a></span></li></ul></li><li><span><a href=\"#Appendix\" data-toc-modified-id=\"Appendix-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Appendix</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "- Review the basics of cleaning the text and visualizing word counts\n",
    "- Introduction to dictionary-based methods and topic models\n",
    "- Text analysis is doable! Much of the code is reusable\n",
    "- Learn limitations and general things to look out for\n",
    "\n",
    "We will step through loading and cleaning a collection of tweets for text analysis. In this session, we will look at dictionary-based text analysis methods like sentiment analysis and topic modeling.\n",
    "\n",
    "<img src=\"imgs/part2/textanalysis_diagrams2.001.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Some operational details:___\n",
    "\n",
    "This session is a [Binder](https://mybinder.org/) instance of a [Jupyter notebook](https://jupyter.org/). This is your own copy of the notebook, where you can edit the code and run it interactively.  \n",
    "\n",
    "The binder session will time out after 10 minutes of inactivity. If this happens, you will need to restart the binder. Reloading the current page will not work. \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/MindyChang/text-analysis-workshop/master?filepath=index.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "### Install packages\n",
    "\n",
    "We only need to install packages once - they have already been installed in this binder.\n",
    "- *tidyverse*\n",
    "    - *dyplr* for dataframe manipulation\n",
    "    - *tidyr* for formatting into tidy data\n",
    "    - *ggplot2* for plotting\n",
    "    - *lubridate* for working with dates and times\n",
    "- *tidytext* for getting text data into a tidy format\n",
    "- *SnowballC* for getting word stems\n",
    "- *stringr* for manipulating strings\n",
    "- *wordcloud* for generating word clouds\n",
    "- *topicmodels* for LDA topic modeling\n",
    "- *stm* for structural topic modeling\n",
    "\n",
    "In the R console, \n",
    "```\n",
    "install.packages(\"tidyverse\")\n",
    "install.packages(\"tidytext\")\n",
    "install.packages(\"SnowballC\")\n",
    "install.packages(\"stringr\")\n",
    "install.packages(\"wordcloud\")\n",
    "install.packages(\"topicmodels\")\n",
    "install.packages(\"stm\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "**Running code interactively**\n",
    "\n",
    "To run a code block, click the gray section and type *shift+enter* (or click the play button on mobile). \n",
    "\n",
    "- The `In [ ]` will indicate the code state. \n",
    "    - `In [*]` means it's currently running. \n",
    "    - `In [5]` means it finished running, where the `[number]` is a running count of the number of blocks you have run.    \n",
    "- After running a block, the cursor will automatically advance to the next block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(tidyr)\n",
    "library(ggplot2)\n",
    "library(lubridate)\n",
    "library(tidytext)\n",
    "library(SnowballC)\n",
    "library(stringr)\n",
    "library(wordcloud)\n",
    "library(topicmodels)\n",
    "library(tm)\n",
    "library(stm)\n",
    "library(quanteda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "\n",
    "<img src=\"imgs/part2/textanalysis_diagrams2.002.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n",
    "\n",
    "### Getting Twitter data\n",
    "- Retrieve data with the Twitter API\n",
    "\n",
    "    - Limited to the last 7-9 days\n",
    "    - Subject to rate limitations\n",
    "    - Twitter may sample or not provide a complete set of tweets in searches\n",
    "    - By default, Twitter will return the most recent tweets that fit the criteria you request\n",
    "\n",
    "- Use an existing Twitter dataset\n",
    "\n",
    "    - Hydrate using the tweet id to retrieve the original tweet info\n",
    "\n",
    "For this workshop, we have an option of 4 different datasets (each with ~4000 tweets), which have already been extracted via the twitter API using the `rtweet` package and saved as a Rdata file.\n",
    "- Trump's tweets collected between 2017-02-05 and 2018-05-18, named `trumptweets`\n",
    "- A collection of tweets that contain the word \"covid\" on 2020-07-29 (spanning 10 minutes), named `covidtweets`\n",
    "- A collection of tweets that contain the phrase \"mental health\" on 2020-07-29 (spanning 3 hours) named `mentalhealthtweets`\n",
    "- A collection of tweets that contain the phrase \"mental health\" on 2020-08-19 (spanning 3 hours) named `mentalhealthtweets2`\n",
    "\n",
    "We will rename the data by assigning the variable name `rawtweets` using the assignment operator `<-` so that all of the following code can work with any Twitter dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title <- 'Mental Health' # other options include: 'Covid', 'Trump', 'Mental Health 2'\n",
    "\n",
    "if (title == 'Trump')\n",
    "{\n",
    "    load(url(\"https://cbail.github.io/Trump_Tweets.Rdata\"))\n",
    "    rawtweets <- trumptweets \n",
    "    keyword <- \"\"\n",
    "} else if (title =='Covid')\n",
    "{\n",
    "    load(\"data/covidtweets.RData\")\n",
    "    rawtweets <- covid_tweets\n",
    "    keyword <- \"covid\"\n",
    "} else if (title == 'Mental Health')\n",
    "{\n",
    "    load(\"data/mentalhealthtweets.RData\")\n",
    "    rawtweets <- mentalhealthtweets\n",
    "    keyword <- \"mental health\"\n",
    "} else if (title == 'Mental Health 2')\n",
    "{\n",
    "    load(\"data/mentalhealthtweets2.RData\")\n",
    "    rawtweets <- mentalhealthtweets2\n",
    "    keyword <- \"mental health\"\n",
    "} else\n",
    "{\n",
    "    warning('Pick a valid dataset from {\"Trump\",\"Covid\",\"Mental Health\"}')\n",
    "}\n",
    "\n",
    "paste(\"Loaded\", title , \"tweet dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data format\n",
    "The data is formatted as a data frame, which is a standard table, where the top row contains column names and each row contains data about a single tweet.\n",
    "\n",
    "- The `head()`function returns a the first few rows of `rawtweets`. \n",
    "\n",
    "    - You can change the number in \n",
    "    `head(rawtweets, #) `\n",
    "    for the number of rows you want to see.\n",
    "\n",
    "Each tweet comes with 90 variables, which are defined in the [tweet data dictionary](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object)\n",
    "\n",
    "Some columns of note:\n",
    "\n",
    "- `status_id` is the unique tweet id\n",
    "- `created_at` contains the timestamp of the tweet\n",
    "- `screen_name` is the twitter handle\n",
    "- `text` contains the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview rawtweets\n",
    "head(rawtweets,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format and clean the text\n",
    "We want to clean the text to keep the most meaningful parts and shape it into the tidy text format for processing.\n",
    "\n",
    "**Token** - a unit of analysis (e.g. words, sequence of words, sentence)\n",
    "\n",
    "**Document** - a unit of context for each token (in this case - a single tweet)\n",
    "\n",
    "**Tidy text format** - One row per token (word in this case) with column variables that have extra context (e.g. which document the word came from)\n",
    "\n",
    "\n",
    "<img src=\"imgs/part2/textanalysis_diagrams2.003.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet-specific cleaning\n",
    "\n",
    "- **Filter out retweets**\n",
    "\n",
    "- **Replace urls and usernames**\n",
    "\n",
    "  - Depending on your goal, sometimes unique words are useful, and sometimes they add to the noise. Here we will abstract away specific urls and twitter handles (usernames).\n",
    "\n",
    "- **Treat the search keyword as one word for easy removal**\n",
    "  - e.g. Mental health --> mentalhealth\n",
    "\n",
    "- **Remove duplicate tweets**\n",
    "  - Some twitter bots repost the same tweet from multiple accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out retweets, find urls and @usernames - replace them with the string \"url\"\n",
    "# regex for parsing tweets\n",
    "url_regex <- \"https?://[^\\\\s]+|&amp;|&lt;|&gt;|\\bRT\\\\b\"\n",
    "user_regex <-\"@[^\\\\s]+\\\\b\"\n",
    "rawtweets <- rawtweets %>%\n",
    "   filter(is_retweet == FALSE) %>%\n",
    "    mutate(text = str_replace_all(text, url_regex, \"url\")) %>%\n",
    "    mutate(text = str_replace_all(text, user_regex, \"username\")) %>%\n",
    "    mutate(text = str_replace_all(text, regex(keyword, ignore_case=TRUE), gsub(\" \",\"\",keyword))) %>%\n",
    "      distinct(text,.keep_all= TRUE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data\n",
    "\n",
    "- The `unnest_tokens()` function separates the tweets into a tidy text format using the token specified. Here we are defining a token as a single word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets<- rawtweets %>%\n",
    "    select(status_id, created_at, text) %>%\n",
    "    unnest_tokens(\"word\", text)\n",
    "\n",
    "head(tidy_tweets, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lowercase\n",
    "Done automatically by the `unnest_tokens` function from the `tidytext` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation\n",
    "Most punctuation is automatically removed by `unnest_tokens` from `tidytext`.\n",
    "\n",
    "Here let's also exclude apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets$word <- gsub(\"\\\\’\",\"\",tidy_tweets$word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "Common words such as “the”, “and”, “for”, “is”, etc. are often described as “stop words,” meaning that they should not be included in a text analysis. \n",
    "- The `tidytext` package has a list of common stop words called `stop_words` that we can use. \n",
    "- There are also some words specific to tweets that we would like to filter out, for example urls and usernames, \"rt\" for retweets, \"t.co\" for twitter link shortening, and \"amp\" for accelerated mobile pages.\n",
    "- There are some words that aren't caught by `stop_words` that we may want to add like \"im\", \"ive\", \"its\"\n",
    "- There are some words in `stop_words` that we may want to keep like \"work\", \"working\", \"worked\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stop_words from tidytext package and remove them from tidy_tweets\n",
    "tweet_words_to_remove <- c(\"url\",\"username\",\"rt\",\"t.co\",\"amp\")\n",
    "other_words_to_remove <- append(c(\"its\",\"im\",\"ive\",\"lot\"), gsub(\" \",\"\",keyword))\n",
    "words_to_keep <- c(\"work\",\"working\",\"worked\")\n",
    "\n",
    "#load stop_words\n",
    "data(\"stop_words\")\n",
    "# add a few more stop words\n",
    "custom_stop_words <- stop_words %>%\n",
    "    bind_rows(tibble(word = append(tweet_words_to_remove, other_words_to_remove),\n",
    "                     lexicon = \"custom\")) %>%\n",
    "    filter(!word %in% words_to_keep)\n",
    "\n",
    "# remove stopwords and other insignificant words from tidy_tweets\n",
    "tidy_tweets <-\n",
    "   tidy_tweets %>%\n",
    "      anti_join(custom_stop_words) \n",
    "\n",
    "print(head(rawtweets$text,2))\n",
    "head(tidy_tweets,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We might need to go back and edit our stopwords later if it removes words that are useful in our context.\n",
    "\n",
    "For example, in Trump tweets:\n",
    "- `tidytext` automatically converted all words to lowercase, and removed \"UN\" \n",
    "- `tidytext` automatically removed punctuation, and \"Secretary-General\" was reduced to only \"secretary\".\n",
    "- removing stopwords removed the word \"working\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers from tidy_tweets\n",
    "tidy_tweets<-tidy_tweets[-grep(\"\\\\b\\\\d+\\\\b\", tidy_tweets$word),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra white spaces from tidy_tweets\n",
    "tidy_tweets$word <- gsub(\"\\\\s+\",\"\",tidy_tweets$word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "We may want to reduce all words to their word stems (or roots).\n",
    "\n",
    "For example: \"unite\", \"united\", \"uniting\", \"unites\" all reduce to \"unit\". \n",
    "\n",
    "But the words \"unit\" and \"units\" also reduce to \"unit\", and these groups have different meanings.\n",
    "\n",
    "The code below shows how to do it using the `snowballC` package, but we won't use it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word stems and save as tidy_tweets_stemmed\n",
    "tidy_tweets_stemmed<-tidy_tweets %>%\n",
    "      mutate_at(\"word\", list(~wordStem((.), language=\"en\")))\n",
    "head(tidy_tweets_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counting \n",
    "Count the most commonly used words across tweets \n",
    "\n",
    "<img src=\"imgs/part2/textanalysis_diagrams2.004.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a word count dataframe\n",
    "Here we create a dataframe with a word column `word` and a count column `n`. \n",
    "\n",
    "Then we sort it from largest to smallest `n` to get the most frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count word frequencies and sort in descending order\n",
    "top_words<-\n",
    "   tidy_tweets %>%\n",
    "    count(word) %>%\n",
    "        arrange(desc(n))\n",
    "head(top_words,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize word frequencies\n",
    "<img src=\"imgs/part2/textanalysis_diagrams2.005.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n",
    "\n",
    "Plot bar charts of word frequencies using `ggplot` from `ggplot2` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 20 most frequently used words\n",
    "# function for bar plots given a dataframe that contains a \"word\" column and a count column (e.g. \"n\")\n",
    "# we pass in: \n",
    "# - unit for the plot title (\"words\", \"bigrams\", \"trigrams\") \n",
    "# - and the name of the count column (e.g. \"n\")\n",
    "plot_frequent_words <- function(word_counts, unit, param) {\n",
    "    options(repr.plot.width=9, repr.plot.height=7)\n",
    "    word_counts %>%\n",
    "        ggplot(aes(x=get(param), y=reorder(word, get(param)), fill=-get(param)))+\n",
    "          geom_bar(stat=\"identity\")+\n",
    "            theme_minimal()+\n",
    "            theme(axis.text.x = element_text(angle = 60, hjust = 1, size=15),\n",
    "                  axis.text.y = element_text(hjust = 1, size=15),\n",
    "                  axis.title = element_text(size=15),\n",
    "                  plot.title = element_text(hjust = 0.5, size=18))+\n",
    "                ylab(\"Frequency\")+\n",
    "                xlab(\"# Occurences\")+\n",
    "                ggtitle(paste(\"Most Frequent\", unit, \"in\", title, \"Tweets\", sep=\" \"))+\n",
    "                guides(fill=FALSE)\n",
    "}\n",
    "\n",
    "top_words %>%\n",
    "  slice(1:20) %>%\n",
    "    plot_frequent_words(\"words\", \"n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds\n",
    "Create wordclouds for qualitative insights using the `wordcloud` function from the `wordcloud` package\n",
    "- `min.freq`: words with frequency below min.freq will not be plotted\n",
    "- `max.words`: Maximum number of words to be plotted. Least frequent terms are dropped\n",
    "- `random.order`: plot words in random order. If false, they will be plotted in decreasing frequency\n",
    "- `rot.per`: proportion words with 90 degree rotation\n",
    "- `colors`: color words from least to most frequent\n",
    "    - choose other color themes from [RColorBrewer](https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a wordcloud \n",
    "set.seed(1234) # for reproducibility \n",
    "wordcloud(words = top_words$word, freq = top_words$n, min.freq = 1,  \n",
    "          max.words=200, random.order=FALSE, rot.per=0.3,colors=brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and n-grams\n",
    "\n",
    "Bigrams and n-grams refer to how text is tokenized, or the size of the unit of analysis.\n",
    "\n",
    "- Unigrams are single words, bigrams are two-word phrases, and n-grams are n-word phrases.\n",
    "\n",
    "**Bigrams**: \n",
    "\n",
    "We tokenize the text into bigrams and then filter out bigrams where either word is a stopword or contains numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams\n",
    "# Preprocess rawtweets by tokenizing into bigrams, removing stopwords from individual words,\n",
    "# and then combining back together\n",
    "tidy_bigrams <-rawtweets %>%\n",
    "    select(status_id,text) %>%\n",
    "        unnest_tokens(output=word, input=text, token = \"ngrams\", n = 2) %>% \n",
    "          separate(word, c(\"word1\", \"word2\"), sep = \" \") %>% \n",
    "              filter(!word1 %in% custom_stop_words$word) %>%\n",
    "              filter(!word2 %in% custom_stop_words$word) %>% \n",
    "              filter(!grepl(\"\\\\d+|\\\\’\", word1))%>%\n",
    "              filter(!grepl(\"\\\\d+|\\\\’\", word2))%>%\n",
    "                  unite(word, word1, word2, sep = \" \")\n",
    "\n",
    "# count bigrams and arrange by frequency\n",
    "top_bigrams <- tidy_bigrams %>%\n",
    "    count(word) %>%\n",
    "        arrange(desc(n))\n",
    "\n",
    "# plot top bigrams\n",
    "top_bigrams %>%\n",
    "  slice(1:20) %>%\n",
    "    plot_frequent_words(\"Bigrams\" , \"n\")\n",
    "\n",
    "set.seed(1234) # for reproducibility \n",
    "wordcloud(words = top_bigrams$word, freq = top_bigrams$n, min.freq = 1,  \n",
    "          max.words=200, random.order=FALSE, rot.per=0.3,colors=brewer.pal(8, \"Dark2\"),scale=c(4,.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trigram**\n",
    "\n",
    "We tokenize the text into trigrams (3-word phrases) and then filter out trigrams where the first or third word is stopwords. We could also filter out trigrams where any of the 3 words are stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams\n",
    "# Preprocess rawtweets by tokenizing into bigrams, removing stopwords from individual words,\n",
    "# and then combining back together\n",
    "tidy_trigrams <-rawtweets %>%\n",
    "    select(status_id,text) %>%\n",
    "        unnest_tokens(output=word, input=text, token = \"ngrams\", n = 3) %>% \n",
    "          separate(word, c(\"word1\", \"word2\", \"word3\"), sep = \" \") %>% \n",
    "              filter(!(word1 %in% custom_stop_words$word)) %>% \n",
    "              #filter(!(word2 %in% custom_stop_words$word)) %>% \n",
    "              filter(!(word3 %in% custom_stop_words$word)) %>%\n",
    "              filter(!grepl(\"\\\\d+|\\\\’\", word1))%>%\n",
    "              filter(!grepl(\"\\\\d+|\\\\’\", word2))%>%\n",
    "              filter(!grepl(\"\\\\d+|\\\\’\", word3))%>%\n",
    "                  unite(word, word1, word2, word3, sep = \" \")%>%\n",
    "                  filter(!grepl(\"NA NA NA\", word)) # tweets shorter than 3 words\n",
    "\n",
    "# count bigrams and arrange by frequency\n",
    "top_trigrams <- tidy_trigrams %>%\n",
    "    count(word) %>%\n",
    "        arrange(desc(n))\n",
    "\n",
    "# plot top bigrams\n",
    "top_trigrams %>%\n",
    "  slice(1:20) %>%\n",
    "    plot_frequent_words(\"Trigrams\",\"n\")\n",
    "\n",
    "# plot wordcloud\n",
    "set.seed(1234) # for reproducibility \n",
    "wordcloud(words = top_trigrams$word, freq = top_trigrams$n, min.freq = 1,  \n",
    "          max.words=100, random.order=FALSE, rot.per=0.3,colors=brewer.pal(8, \"Dark2\"),scale=c(2,.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf: Term Frequency Inverse Document Frequency\n",
    "A statistic for how important a word is to a document in a collection\n",
    "\n",
    "Words that occur more frequently in one document (tweet) and less frequently in other documents should be given more importance as they are more useful for classification. This method is better suited for longer documents.\n",
    "\n",
    "***Term frequency***\n",
    "\n",
    "$tf(term)=\\displaystyle(\\frac{n_{occurences\\ of\\ term\\ in\\ document}}{n_{words\\ in\\ document}})$\n",
    "\n",
    "***Inverse document Frequency:***\n",
    "\n",
    "$idf(term)=\\displaystyle log(\\frac{n_{documents}}{n_{documents\\ containing\\ term}})$\n",
    "\n",
    "The `bind_tf_idf` function from the `tidytext` package calculates the tf-idf value for each token (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets_tfidf <- tidy_tweets %>%\n",
    "    count(word, status_id) %>%\n",
    "        bind_tf_idf(word, status_id, n) %>%\n",
    "            distinct(word,.keep_all = TRUE) %>%\n",
    "                arrange(desc(tf_idf))\n",
    "\n",
    "head(tidy_tweets_tfidf,8)\n",
    "\n",
    "#set.seed(1234) # for reproducibility     \n",
    "#wordcloud(words = tidy_tweets_tfidf$word, freq = tidy_tweets_tfidf$tf_idf, min.freq = max(tidy_tweets_tfidf$tf_idf)/10,  \n",
    "#          max.words=100, random.order=FALSE, rot.per=0.3,colors=brewer.pal(8, \"Dark2\"),scale=c(2,.5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary-based text analysis\n",
    "Dictionary-based techniques involve taking a words that have been assigned a particular meaning or value and counting the number of occurences in each document. This approach assumes each word has an intrinsic meaning and does not take into account the context of each word.\n",
    "\n",
    "<img src=\"imgs/part2/textanalysis_diagrams2.006.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select for keywords\n",
    "A simple example is extracting tweets that contain a certain word or list of words. \n",
    "\n",
    "We can create a custom list of words and find tweets that contain any of those words\n",
    "\n",
    "The `str_detect` function from `stringr` package finds all text that contains a specified string query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of words as our custom dictionary\n",
    "custom_dictionary<-c(\"college\",\"student\") #c(\"school\") \n",
    "\n",
    "# extract tweets that contain any words from our custom dictionary\n",
    "custom_dictionary_tweets<-rawtweets[str_detect(rawtweets$text, \n",
    "                                                regex(paste(custom_dictionary, collapse=\"|\"),\n",
    "                                                      ignore_case=TRUE)),]\n",
    "paste (nrow(custom_dictionary_tweets), 'tweets found')\n",
    "custom_dictionary_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top words from tweets that match our custom dictionary\n",
    "custom_top_words<-custom_dictionary_tweets %>%\n",
    "    select(status_id,text) %>%\n",
    "      unnest_tokens(\"word\", text) %>%        # tokenize\n",
    "        anti_join(custom_stop_words) %>%     # remove stopwords\n",
    "        filter(!grepl(\"\\\\d+|\\\\’\", word))%>%  # remove numbers and apostrophes\n",
    "        mutate_at(\"word\", list(~wordStem((.), language=\"en\"))) %>% # get word stems\n",
    "            count(word) %>%\n",
    "                arrange(desc(n))\n",
    "\n",
    "#plot unigram wordcloud\n",
    "set.seed(1234) # for reproducibility \n",
    "wordcloud(words = custom_top_words$word, freq = custom_top_words$n, min.freq = 1,  \n",
    "          max.words=100, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"),scale=c(4,1))\n",
    "\n",
    "# get top bigrams from tweets that match our custom dictionary\n",
    "custom_top_bigrams <-custom_dictionary_tweets %>%\n",
    "    select(status_id,text) %>%\n",
    "        unnest_tokens(output=word, input=text, token = \"ngrams\", n = 2) %>% \n",
    "          separate(word, c(\"word1\", \"word2\"), sep = \" \") %>% \n",
    "              filter(!word1 %in% custom_stop_words$word) %>%\n",
    "              filter(!word2 %in% custom_stop_words$word) %>% \n",
    "              filter(!grepl(\"\\\\d+|\\\\’\", word1))%>%\n",
    "              filter(!grepl(\"\\\\d+|\\\\’\", word2))%>%\n",
    "                  unite(word, word1, word2, sep = \" \") %>%\n",
    "                    count(word) %>%\n",
    "                    arrange(desc(n))\n",
    "\n",
    "# plot bigram wordcloud\n",
    "set.seed(1234) # for reproducibility \n",
    "wordcloud(words = custom_top_bigrams$word, freq = custom_top_bigrams$n, min.freq = 1,  \n",
    "          max.words=100, random.order=FALSE, rot.per=0.3,colors=brewer.pal(8, \"Dark2\"),scale=c(2,.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis\n",
    "One popular type of dictionary is a sentiment dictionary which can be used to assess the valence of a given text by searching for words that describe affect or opinion. \n",
    "Sentiment dictionaries can vary highly. See [this paper]( https://homepages.dcc.ufmg.br/~fabricio/download/cosn127-goncalves.pdf) for a comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a dictionary\n",
    "\n",
    "`tidytext` has a few built-in sentiment dictionaries\n",
    "- `afinn` - sentiment words in twitter discussions of climate change (value between -5 and 5)\n",
    "- `bing` - sentiment words identified in online forums (negative vs positive)\n",
    "- `nrc` - emotional valence words labeled by mturk workers\n",
    "    - Words in this dictionary are labeled with the sentiments:\n",
    "    \"negative\",\"positive\",\"trust\",\"fear\",\"sadness\",\"anger\", \"surprise\",\"disgust\",\"joy\",\"anticipation\"\n",
    "    - Each word can be associated with multiple sentiments\n",
    "\n",
    "`bing` is the only dictionary available in Binder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the bing dictionary\n",
    "sentiment_dictionary <- \"bing\"\n",
    "unique_sentiments <- unique(get_sentiments(sentiment_dictionary)$sentiment)\n",
    "unique_sentiments\n",
    "head(get_sentiments(sentiment_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count sentiment words across tweets\n",
    "- We use the `inner_join` function to count the number of sentiment words used across all tweets\n",
    "\n",
    "- `count(word, sentiment)` counts the number of times each word + sentiment pair appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_by_word <- tidy_tweets %>%\n",
    "  inner_join(get_sentiments(sentiment_dictionary)) %>%\n",
    "    count(word, sentiment) \n",
    "\n",
    "head(sentiments_by_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the top 10 positive and negative words used across all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (nrow(distinct(get_sentiments(sentiment_dictionary),sentiment))>2){\n",
    "    options(repr.plot.width=20, repr.plot.height=15)\n",
    "}else{\n",
    "    options(repr.plot.width=15, repr.plot.height=7)\n",
    "}\n",
    "sentiments_by_word %>%\n",
    "  group_by(sentiment) %>%\n",
    "  top_n(10) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(word = reorder(word, n)) %>%\n",
    "  ggplot(aes(word, n, fill = sentiment)) +\n",
    "    theme_minimal()+\n",
    "    theme(axis.text.x = element_text(angle = 60, hjust = 1, size=15),\n",
    "          axis.text.y = element_text(hjust = 1, size=15),\n",
    "          axis.title = element_text(size=15),\n",
    "          plot.title = element_text(hjust = 0.5, size=18),\n",
    "          strip.text = element_text(size=18))+\n",
    "          \n",
    "    theme(aspect.ratio = 1.5/1)+\n",
    "    geom_col(show.legend = FALSE, width = 0.5) +  \n",
    "    facet_wrap(~sentiment, scales = \"free_y\") +\n",
    "    labs(y = \"Contribution to sentiment\",\n",
    "         x = NULL) +\n",
    "    coord_flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In Trump and covid tweets, \"trump\" happens to be a positive sentiment word, but it's being used in a different way. Context-appropriate dictionaries will give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count sentiments by tweet\n",
    "Now let's use the `inner_join` function and count the number of sentiment words per tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of sentiment words for each tweet\n",
    "tweet_sentiments <- tidy_tweets %>%\n",
    "  inner_join(get_sentiments(sentiment_dictionary)) %>%\n",
    "    count(status_id, sentiment) \n",
    "\n",
    "head(tweet_sentiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine sentiment labels\n",
    "\n",
    "Each tweet can have any number of positive or negative words. Let's see what the tweets with the highest counts for each sentiment look like.\n",
    "\n",
    "1) Arrange a dataframe where each row has the full tweet and count of sentiment words\n",
    "\n",
    "  - The `spread` function spreads a key-value pair across columns into multiple columns with the key as the column name.\n",
    "\n",
    "2) List the tweets with the highest counts for each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group sentiment counts by tweet\n",
    "tweet_sentiments_spread <-tweet_sentiments %>%\n",
    "  spread(sentiment, n, fill=0)\n",
    "\n",
    "# add in the original text and favorite count\n",
    "tweet_sentiments_full <- merge(tweet_sentiments_spread, \n",
    "                              rawtweets[c(\"status_id\", \"created_at\", \"text\",\"favorite_count\")], \n",
    "                              by=\"status_id\")\n",
    "#head(tweet_sentiments_full,4)\n",
    "\n",
    "# for each sentiment, print a tweet with the highest count for that sentiment\n",
    "sentiments <- as.list(get_sentiments(sentiment_dictionary) \n",
    "                      %>% distinct(sentiment))\n",
    "\n",
    "high_sentiments <- tweet_sentiments_full[FALSE,] \n",
    "high_sentiments$highest <- NULL \n",
    "\n",
    "for (s in sentiments[[1]])\n",
    "{\n",
    "    tmp <- tweet_sentiments_full %>% \n",
    "            arrange(desc(get(s)))\n",
    "    tmp$highest <- s\n",
    "    high_sentiments <- high_sentiments %>%\n",
    "        bind_rows(tmp[1:2,])\n",
    "}\n",
    "high_sentiments <- high_sentiments %>%\n",
    "    select(-one_of('status_id', 'favorite_count'))\n",
    "high_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate wordclouds for the most commonly used words in \n",
    "\n",
    "- the top 50 tweets containing the most negative sentiment words and \n",
    "- the top 50 tweets containing the most positive sentiment words. \n",
    "\n",
    "This wordcloud will include non-sentiment words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word clouds for the top 50 most negative and top 50 most positive tweets\n",
    "for (s in c(\"negative\",\"positive\"))\n",
    "{\n",
    "    print(s)\n",
    "    top_words <- tweet_sentiments_full %>% \n",
    "        arrange(desc(get(s))) %>%              # order by sentiment count\n",
    "        slice(1:50) %>%                        # take the top 50\n",
    "        select(status_id,text) %>%\n",
    "          unnest_tokens(\"word\", text) %>%      # tokenize into words\n",
    "            anti_join(custom_stop_words) %>%   # remove stopwords\n",
    "              filter(!grepl(\"\\\\d+\", word))%>%  # filter out numbers\n",
    "                filter(!grepl(\"\\\\’\", word))%>% # filter out apostrophes\n",
    "                   count(word) %>%\n",
    "                     arrange(desc(n))\n",
    "\n",
    "    #plot wordcloud)\n",
    "    set.seed(1234) # for reproducibility \n",
    "    wordcloud(words = top_words$word, freq = top_words$n, min.freq = 1,  \n",
    "              max.words=100, random.order=FALSE, rot.per=0.3, colors=brewer.pal(8, \"Dark2\"))#,scale=c(4,1))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [extra] Plot sentiments over time\n",
    "This applies to the Trump dataset, which has a timespan of over one day.\n",
    "A note about timestamps: The `lubridate` package provides functions like `as.Date` and `ymd_hms` to deal with timestamp formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert timestamps to timestamp format\n",
    "#rawtweets$created_at <- ymd_hms(rawtweets$created_at)\n",
    "\n",
    "## examples for retriving tweets at specific dates/times: \n",
    "# rawtweets[as.Date(rawtweets$created_at) == as.Date(\"2018-05-18\"),]\n",
    "# rawtweets[rawtweets$created_at == ymd_hms(\"2017-05-05 19:43:37\"),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get date for each tweet\n",
    "tweet_sentiments_full$date <- as.Date(tweet_sentiments_full$created_at, format=\"%Y-%m-%d %x\")\n",
    "\n",
    "# plot count of negative and positive sentiment words by date\n",
    "options(repr.plot.width=12, repr.plot.height=10)\n",
    "tweet_sentiments_full %>%\n",
    "    group_by(date) %>%\n",
    "    summarise_at(vars(positive,negative), list(n = sum)) %>%\n",
    "    ggplot(aes(x = date)) + \n",
    "      geom_line(aes(y = positive_n, color = \"Positive\")) +\n",
    "      geom_line(aes(y = negative_n, color = \"Negative\")) + \n",
    "        scale_color_manual(values = c(\n",
    "            'Positive' = 'green',\n",
    "            'Negative' = 'red')) +\n",
    "        theme_minimal()+\n",
    "        theme(axis.text.x = \n",
    "            element_text(angle = 60, hjust = 1, size=13))+\n",
    "        theme(plot.title = \n",
    "            element_text(hjust = 0.5, size=18))+\n",
    "          ylab(\"Number of Sentiment Words\")+\n",
    "          xlab(\"\")+\n",
    "          labs(color = \"Sentiment\") +\n",
    "          ggtitle(paste(\"Positive and Negative Sentiment in\", title, \"Tweets\", sep=\" \"))+\n",
    "          theme(aspect.ratio=1/4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [extra] Linear model for favorites based on sentiment count\n",
    "This applies to the Trump dataset, which has a substantial favorite count due to being popular and spanning enough time to gather favorites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 <-tweet_sentiments_full %>%\n",
    "    lm(data=., favorite_count ~  negative + positive)\n",
    "    ## for nrc only\n",
    "    #lm(data=., favorite_count ~  negative + positive + disgust +sadness + trust + fear + joy + anticipation)\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic Inquiry Word Count (LIWC)\n",
    "\n",
    "A large, commonly used, purchasable dictionary developed by a social psychologist and team to classify different types of psychometric properties and substantive properties of a text. This dictionary was built in a systematic way rather than through empirically labeling texts and has been psychometrically validated. \n",
    "\n",
    "More details [here](http://liwc.wpengine.com/how-it-works/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models\n",
    "\n",
    "A method for analyzing “bags” or groups of words together instead of counting them individually.\n",
    "- Every document is a mixture of topics: Assume there are a certain number of latent or underlying themes / \"topics\"\n",
    "- Every topic is a mixture of words: Topics consist of groups of words that tend to co-occur\n",
    "\n",
    "In practice:\n",
    "- Choosing the appropriate number of topics (k) is difficult and kind of an art - labeling the topics requires human interpretation\n",
    "- Topic modeling works best for not-too-short text (>50 words) with consistent structure - short text topic modeling is an area of active research\n",
    "\n",
    "<img src=\"imgs/part2/textanalysis_diagrams2.007.png\" alt=\"Sentiment analysis flow diagram\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (Latent dirichlet allocation)\n",
    "LDA is the most common form of topic modeling, using an iterative algorithm to update the prevalence of each word across the k topics and the prevalence of the topics in each document. This uses the Term Frequency-Inverse Document Frequency metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document-term matrix \n",
    "The `topicmodels` package requires the data to be formatted as a document-term matrix, where each row is a document, each column is a term, and each cell contains the count of the given term in the given document.\n",
    "\n",
    "We can pass in any of our tidy text (`tidy_tweets`, `tidy_bigrams`, `tidy_trigrams`) to be converted into the document-term matrix format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to document term matrix\n",
    "tidy_tweets_DTM<-\n",
    "  tidy_bigrams %>%\n",
    "  count(status_id, word) %>%\n",
    "  cast_dtm(status_id, word, n)\n",
    "\n",
    "inspect(tidy_tweets_DTM[1:4,1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LDA topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topic models\n",
    "k_topics = 10\n",
    "tweets_topic_model<-LDA(tidy_tweets_DTM, k=k_topics, control = list(seed = 321))\n",
    "paste(\"Created a topic model with\", k_topics , \"topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top terms in each topic\n",
    "Each term is assigned a value beta for each topic, representing the probability of the term occuring in a given topic.\n",
    "\n",
    "Let's look at the top terms in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframe with beta value for each term and topic combination\n",
    "topic_terms <- tidy(tweets_topic_model, matrix = \"beta\")\n",
    "\n",
    "# choose number of terms to show per topic\n",
    "n_top = 10\n",
    "\n",
    "# extract top terms in each topic\n",
    "top_topic_terms <- \n",
    "  topic_terms %>%\n",
    "  group_by(topic) %>%\n",
    "    top_n(n_top, beta) %>%\n",
    "    slice(1:n_top)%>%\n",
    "  ungroup() %>%\n",
    "  arrange(topic, beta)\n",
    "\n",
    "# plot the top terms for each topic\n",
    "if (k_topics>12){\n",
    "    options(repr.plot.width=20, repr.plot.height=15)\n",
    "}else{\n",
    "    options(repr.plot.width=20, repr.plot.height=10)\n",
    "}\n",
    "\n",
    "top_topic_terms %>%\n",
    "    mutate(topic = paste0(\"Topic \", topic),\n",
    "           term = reorder_within(term, beta, topic)) %>%\n",
    "    ggplot(aes(term, beta, fill = as.factor(topic))) +\n",
    "        geom_col(alpha = 0.8, show.legend = FALSE) +\n",
    "        facet_wrap(~ topic, scales = \"free_y\") +\n",
    "        coord_flip() +\n",
    "        scale_x_reordered() +\n",
    "\n",
    "        theme_minimal()+\n",
    "        theme(axis.text.x = element_text(angle=60, hjust = 1, size=14),\n",
    "              axis.text.y = element_text(hjust = 1, size=14),\n",
    "              strip.text = element_text(size=14),)+\n",
    "        \n",
    "        labs(x = NULL, y = expression(beta),\n",
    "             title = \"Highest word probabilities for each topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top documents in each topic\n",
    "Each document is assigned a value gamma for each topic, representing the probability of the document being about a given topic.\n",
    "\n",
    "Let's read some of the tweets that score high in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataframe with gamma value for each document and topic combination\n",
    "document_topics <- tidy(tweets_topic_model, matrix = \"gamma\")\n",
    "\n",
    "# choose number of documents to show per topic\n",
    "n_top = 5\n",
    "\n",
    "# read the top documents for each topic\n",
    "top_topic_documents <- document_topics %>%\n",
    "    group_by(topic) %>%\n",
    "    top_n(n_top, gamma) %>%\n",
    "    slice(1:n_top) %>%\n",
    "    ungroup() %>%\n",
    "    rename(\"status_id\" = \"document\") %>%\n",
    "    merge(rawtweets[c(\"status_id\", \"text\")], by=\"status_id\", how=\"left\") %>%\n",
    "    arrange(topic,desc(gamma))\n",
    "\n",
    "top_topic_documents\n",
    "\n",
    "## view all documents from a particular topic\n",
    "#document_topics %>%\n",
    "#    filter(topic==3) %>%\n",
    "#    rename(\"status_id\" = \"document\") %>%\n",
    "#    merge(rawtweets[c(\"status_id\", \"text\")], by=\"status_id\", how=\"left\") %>%\n",
    "#    arrange(desc(gamma))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STM (Structural Topic Modeling)\n",
    "\n",
    "Structural topic modeling is a form of topic modeling that uses meta data about documents (e.g. author name, date, etc) to improve the assignment of words to latent topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "Combine 2 datasets - tweets that contain the phrase \"mental health\"\n",
    "- dataset 1:  4000 tweets collected on 2020-07-30 \n",
    "- dataset 2:  4000 tweets collected on 2020-08-19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine mental health tweets and add dataset column to indicate source\n",
    "load(\"data/mentalhealthtweets.RData\")\n",
    "load(\"data/mentalhealthtweets2.RData\")\n",
    "mentalhealthtweets$dataset = 1\n",
    "mentalhealthtweets2$dataset = 2\n",
    "rawtweets2 <- rbind(mentalhealthtweets, mentalhealthtweets2)\n",
    "\n",
    "# pre-process the raw text    \n",
    "url_regex <- \"https?://[^\\\\s]+|&amp;|&lt;|&gt;|\\bRT\\\\b\"\n",
    "user_regex <-\"@[^\\\\s]+\\\\b\"\n",
    "rawtweets2 <- rawtweets2 %>%\n",
    "    mutate(text = str_replace_all(text, url_regex, \"url\")) %>%\n",
    "    mutate(text = str_replace_all(text, user_regex, \"username\")) %>%\n",
    "    mutate(text = str_replace_all(text, regex(keyword, ignore_case=TRUE), gsub(\" \",\"\",keyword))) %>%\n",
    "    distinct(text,.keep_all= TRUE)\n",
    "paste(\"loaded 2 mental health datasets into rawtweets2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the data\n",
    "There are a few ways to get to the data format used for the `stm` package\n",
    "\n",
    "- Create tidy text -> summarize text -> create sparse data format\n",
    "- Create tidy text -> summarize text -> use quanteda package to convert to dfm (document-term matrix)\n",
    "- Use `stm` package's built-in text processing from raw data (but cannot create n-grams) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tidy bigrams\n",
    "tidy_bigrams2 <-rawtweets2 %>%\n",
    "    select(dataset,status_id,text) %>%\n",
    "        unnest_tokens(output=word, input=text, token = \"ngrams\", n = 2) %>% \n",
    "          separate(word, c(\"word1\", \"word2\"), sep = \" \") %>% \n",
    "              filter(!word1 %in% custom_stop_words$word) %>%\n",
    "              filter(!word2 %in% custom_stop_words$word) %>% \n",
    "              filter(!grepl(\"\\\\d+|\\\\’\", word1))%>%\n",
    "              filter(!grepl(\"\\\\d+|\\\\’\", word2))%>%\n",
    "                  unite(word, word1, word2, sep = \" \") \n",
    "\n",
    "# summarize bigrams by counts\n",
    "bigrams_count <- tidy_bigrams2 %>%\n",
    "                    count(dataset,status_id,word) %>%\n",
    "                    arrange(desc(n))\n",
    "\n",
    "# create sparse data format for stm\n",
    "bigrams_sparse <- bigrams_count %>%\n",
    "    cast_sparse(status_id, word, n)\n",
    "bigrams_covariates <- tidy_bigrams2 %>%\n",
    "    sample_frac() %>%\n",
    "    distinct(status_id,dataset) %>%\n",
    "    merge(rawtweets2[c(\"status_id\", \"text\")], by=\"status_id\", how=\"left\")\n",
    "    \n",
    "\n",
    "##alternative1: use quanteda package to convert summarized text to document-feature(term) matrix using quanteda\n",
    "#bigrams_dfm <- bigrams_count %>%\n",
    "#    cast_dfm(status_id, word, n)\n",
    "#\n",
    "#bigrams_stm_data <- convert(bigrams_dfm, to = \"stm\", docvars = bigrams_covariates)\n",
    "#docs <- bigrams_stm_data$documents \n",
    "#vocab <- bigrams_stm_data$vocab    \n",
    "#meta <- bigrams_stm_data$meta  \n",
    "\n",
    "\n",
    "###alternative2: use STM's built in packages\n",
    "## directly process raw tweets using the text processing built in to stm, but it can't create ngrams\n",
    "## preprocess the text \n",
    "#custom_stopwords = c(\"rt\",\"t.co\",\"amp\",\"ive\",\"im\",\"its\",\"mh\",\"url\",\"username\")\n",
    "#custom_punctuation = c(\"\\\\’\")\n",
    "#processed <- textProcessor(rawtweets2$text, metadata = rawtweets2, \n",
    "#                           customstopwords = custom_stopwords,\n",
    "#                           custompunctuation = custom_punctuation)\n",
    "## create output files\n",
    "#out <- prepDocuments(processed$documents, processed$vocab, processed$meta)\n",
    "#docs <- out$documents\n",
    "#vocab <- out$vocab\n",
    "#meta <- out$meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create STM model\n",
    "Creating the model takes several minutes to run. We have a few pre-saved models run with k = 10, 15, or 20 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_topics_stm = 10 #saved models: 10, 15, 20\n",
    "load_file = TRUE\n",
    "\n",
    "# load the pre-run model or run one yourself\n",
    "if (load_file){\n",
    "   load(paste0(\"models/bigram_stm_\",k_topics_stm,\".RData\"))  \n",
    "}else\n",
    "{\n",
    "    bigram_stm <- stm(bigrams_sparse, \n",
    "                      K = k_topics_stm, \n",
    "                      prevalence = ~ dataset,\n",
    "                      data = bigrams_covariates,\n",
    "                      verbose = FALSE,\n",
    "                      max.em.its = 75,\n",
    "                      init.type = \"Spectral\")\n",
    "}\n",
    "\n",
    "# plot the top terms in each topic\n",
    "if (k_topics_stm>=15){\n",
    "    options(repr.plot.width=20, repr.plot.height=k_topics_stm)\n",
    "}else{\n",
    "    options(repr.plot.width=20, repr.plot.height=10)\n",
    "}\n",
    "plot(bigram_stm, n=6)\n",
    "#labelTopics(bigram_topic_model, n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top documents in each topic\n",
    "Let's read some of the tweets that score high in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thoughts <- findThoughts(bigram_stm,texts=bigrams_covariates$text, topics=1:k_topics_stm, n=4)\n",
    "thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of dataset (date) on topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_topics<-estimateEffect(formula = 1:k_topics_stm ~ dataset, stmobj = bigram_stm, metadata = bigrams_covariates, uncertainty = \"Global\")\n",
    "\n",
    "# plot the top terms for each topic\n",
    "if (k_topics_stm>=15){\n",
    "    options(repr.plot.width=20, repr.plot.height=k_topics_stm)\n",
    "}else{\n",
    "    options(repr.plot.width=20, repr.plot.height=12)\n",
    "}\n",
    "plot(predict_topics, covariate = \"dataset\", topics = 1:k_topics_stm,\n",
    " model = bigram_stm, method = \"difference\",\n",
    " cov.value1 = \"2\", cov.value2 = \"1\",\n",
    " xlab = \"More 2020-07-30 ... More 2020-08-19\",\n",
    " main = \"2020-07-30 dataset vs. 2020-08-19 dataset\",\n",
    " xlim = c(-.4, .4), n=4, labeltype=\"prob\"\n",
    "     #labeltype = \"custom\", custom.labels = c('Topic 3', 'Topic 5','Topic 9'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic model notes\n",
    "\n",
    "\n",
    "- [keywordATM](https://arxiv.org/pdf/2004.05964.pdf): specify topics and associated keywords before modeling [[keywordATM package](https://keyatm.github.io/keyATM/)]\n",
    "\n",
    "- [Biterm topic models](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.4032&rep=rep1&type=pdf): learn unordered word groupings across the entire document for short texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Slang and emojis add a layer of complexity to text analysis\n",
    "- [[Paper] Artificial Intelligence and\n",
    "Inclusion: Formerly Gang Involved Youth as Domain\n",
    "Experts for Analyzing\n",
    "Unstructured Twitter Data](https://safelab.socialwork.columbia.edu/sites/default/files/content/AI%26Inclusion.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "507.778px",
    "left": "23px",
    "top": "312.052px",
    "width": "284.41px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
